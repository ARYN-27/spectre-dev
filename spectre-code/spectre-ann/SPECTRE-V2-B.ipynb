{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECTRE-CPU-V2\n",
    "> Trained with **CICIDS2017**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP PRE-REQUISITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 04:37:27.484433: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-15 04:37:27.620824: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-15 04:37:27.621516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-15 04:37:28.538410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "from keras import layers, models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.model_selection import train_test_split # For basic dataset splitting\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "\n",
    "def escape():\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-6.3.1-zen2-1-zen-x86_64-with-glibc2.37\n",
      "Tensor Flow Version: 2.12.0\n",
      "Keras Version: 2.12.0\n",
      "\n",
      "Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n",
      "[GCC 11.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT SETUP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup INFO level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of permutations to run. Can be altered for needs.\n",
    "number_of_permutations = 100\n",
    "\n",
    "# 10 folds is usually the heuristic to follow for larger datasets of around this size.\n",
    "num_of_splits_for_skf = 10\n",
    "\n",
    "# Seed value to pass into models so that repeated runs result in the same output\n",
    "seed_val = 1\n",
    "\n",
    "# Number of statistical distance measures to run (for the results, columns section)\n",
    "num_of_statistical_dist_measures = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN TRAINING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DDoS Dataset - Kaggle**\n",
    "- Use DASK module for large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDoS_Kaggle = pd.read_csv('../../dataset/DDoS_Dataset/ddos_balanced/final_dataset.csv')\n",
    "#df = DDoS_Kaggle.copy()\n",
    "#df.head()\n",
    "\n",
    "npartitions = 10  # Adjust this value based on your available RAM\n",
    "DDoS_Kaggle = dd.read_csv('../../dataset/DDoS_Dataset/ddos_balanced/final_dataset.csv', assume_missing=True, blocksize='64MB', npartitions=npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = DDoS_Kaggle.map_partitions(clean_dataset)\n",
    "df = df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n",
    "# Only 7 principal components needed when using non-normalised PCA dataset.\n",
    "dimensions_num_for_PCA = 7\n",
    "\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)  # Replace np.inf and -np.inf with np.nan\n",
    "    df.dropna(inplace=True)  # Drop rows containing np.nan\n",
    "    return df\n",
    "\n",
    "def get_PCA_feature_names(num_of_pca_components):\n",
    "    feature_names = []\n",
    "    for i in range(num_of_pca_components):    \n",
    "        feature_names.append(f\"Principal component {i+1}\")\n",
    "    return feature_names\n",
    "\n",
    "# Renaming columns and creating a copy\n",
    "df = DDoS_Kaggle.copy()\n",
    "df = df.rename(columns=lambda x: x.strip().lower().replace(' ', '_').replace('(', '').replace(')', ''))\n",
    "df_cleaned = clean_dataset(df).compute()\n",
    "\n",
    "# Resetting index and removing unneeded index column\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving the label attribute before dropping it\n",
    "df_labels = df_cleaned['label']\n",
    "df_no_labels = df_cleaned.drop('label', axis=1)\n",
    "df_features = df_no_labels.columns.tolist()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_no_labels)\n",
    "df_scaled = pd.DataFrame(data=df_scaled, columns=df_features)\n",
    "\n",
    "# Performing PCA\n",
    "dimensions_num_for_PCA = 2  # You need to define dimensions_num_for_PCA\n",
    "pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Creating a DataFrame with principal components\n",
    "principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)\n",
    "df_pc = pd.DataFrame(data=principal_components, columns=principal_component_headings)\n",
    "\n",
    "# Concatenating principal components and labels\n",
    "df_final = pd.concat([df_pc, df_labels], axis=1)\n",
    "\n",
    "# Applying LabelBinarizer to the labels\n",
    "lb = LabelBinarizer()\n",
    "df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "\n",
    "# Displaying the final DataFrame\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation and Stratified splitting**\n",
    "\n",
    "Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the label so that the answers aren't provided to the model, in training.\n",
    "X = df_final.drop(['label'], axis = 1)\n",
    "y = df_final['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\n",
    "skf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, splitting the data into train and test data, using the optimal splitting techniques of K-Fold and Stratified Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "    \n",
    "print( 'X_train length: ', len(X_train) ) # To check if splits worked\n",
    "print( 'y_train length: ', len(y_train) )\n",
    "print( 'X_test length: ', len(X_test) )\n",
    "print( 'y_test length: ', len(y_test) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Define the ANN model\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(512, kernel_initializer='he_normal', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(256, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(64, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(32, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, kernel_initializer='he_normal', input_shape=(X_train.shape[1],), kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate performance metrics\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as SavedModel\n",
    "tf.saved_model.save(model, '/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/B/SavedModel/')\n",
    "\n",
    "# Export as Keras Model\n",
    "model.save(\"/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/A/spectre_ddos_2_B_hd5\")\n",
    "\n",
    "# Export as Keras H5 Model\n",
    "model.save(\"/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/A/spectre_ddos_2_B_h5.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
