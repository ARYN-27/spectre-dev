{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECTRE PROTOTYPE\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `producer.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`producer.py` is a Python script that performs the following tasks:\n",
    "\n",
    "1. Import necessary libraries and modules. \n",
    "2. Print a welcome message and wait for 1 second. \n",
    "3. Define the prod_datapreprocess function, which performs the following tasks: \n",
    "   1. Read a CSV file and create a DataFrame.\n",
    "   2. Clean the dataset by removing NaN, inf, and -inf values.\n",
    "   3. Perform feature scaling using StandardScaler.\n",
    "   4. Apply PCA (Principal Component Analysis) to reduce dimensionality.\n",
    "   5. Combine the principal components with the original labels.\n",
    "   6. Perform label binarization for the 'label' column.\n",
    "   7. Return the processed DataFrame with features (X) and labels (y).\n",
    "4. Call the prod_datapreprocess function on a given dataset.\n",
    "5. Configure the Kafka producer with the necessary settings.\n",
    "6. Create a Kafka producer instance.\n",
    "7. Configure and create a Kafka consumer for handshake purposes.\n",
    "8. Perform a handshake between the producer and consumer, waiting for a 'READY' message.\n",
    "9. Send a 'READY' message from the producer to the consumer.\n",
    "10. Iterate through the processed DataFrame (X) and send each row to the Kafka producer.\n",
    "11. Flush the producer to ensure all messages are sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# producer.py\n",
    "# https://www.phind.com/search?cache=cf139efb-38e8-4fb5-9cda-5c67194a11a6\n",
    "\n",
    "# Import necessary libraries\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "# Print the welcome message\n",
    "print(\"==================================\")\n",
    "print(\"SPECTRE - PRODUCER MODULE\")\n",
    "print(\"==================================\")\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Define a function to preprocess the data\n",
    "def prod_datapreprocess(csv_file):\n",
    "    \n",
    "    # Read a CSV file and create a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    dimensions_num_for_PCA = 7\n",
    "    \n",
    "    # Function to clean the dataset by removing NaN, inf, and -inf values\n",
    "    def clean_dataset(df):\n",
    "        assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "        df.dropna(inplace=True)\n",
    "        indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "        return df[indices_to_keep]\n",
    "\n",
    "    # Function to get PCA feature names\n",
    "    def get_PCA_feature_names(num_of_pca_components):\n",
    "        feature_names = []\n",
    "        for i in range(num_of_pca_components):\n",
    "            feature_names.append(f\"Principal component {i+1}\")\n",
    "        return feature_names\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = clean_dataset(df_cleaned)\n",
    "\n",
    "    df_cleaned = df_cleaned.reset_index()\n",
    "    df_cleaned.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Saving the label attribute before dropping it\n",
    "    df_labels = df_cleaned['label']\n",
    "    df_cleaned.drop('label', axis=1, inplace=True)\n",
    "    df_features = df_cleaned.columns.tolist()\n",
    "\n",
    "    # Perform feature scaling\n",
    "    df_scaled = StandardScaler().fit_transform(df_cleaned)\n",
    "    df_scaled = pd.DataFrame(data=df_scaled, columns=df_features)\n",
    "\n",
    "    # Performing PCA\n",
    "    pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Creating a DataFrame with principal components\n",
    "    principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)\n",
    "    df_pc = pd.DataFrame(data=principal_components, columns=principal_component_headings)\n",
    "\n",
    "    # Combine the principal components with the original labels\n",
    "    df_final = pd.concat([df_pc, df_labels], axis=1)\n",
    "\n",
    "    # Perform label binarization. Converts \"ANOMALY\" = 1 and \"BENIGN\" = 0.\n",
    "    lb = LabelBinarizer()\n",
    "    df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "\n",
    "    # Split the dataset into features (X) and labels (y)\n",
    "    X = df_final.drop(['label'], axis = 1)\n",
    "    y = df_final['label']\n",
    "\n",
    "    # Returns features(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "# Read the CSV file and preprocess the data\n",
    "\n",
    "# DDoS Attack CSV\n",
    "X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "\n",
    "# DDoS Prime CSV\n",
    "#X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/DDoS_Dataset/ddos_balanced/final_dataset.csv')\n",
    "\n",
    "# Bening CSV\n",
    "#X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "\n",
    "# Set the producer configuration\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'max.in.flight.requests.per.connection': 1   # Add this line to set the maximum number of in-flight messages to 1\n",
    "}\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Set the handshake consumer configuration\n",
    "handshake_consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'producer_handshake_group',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer for the handshake\n",
    "handshake_consumer = Consumer(handshake_consumer_conf)\n",
    "handshake_consumer.subscribe(['handshake'])\n",
    "\n",
    "# Wait for the handshake from the consumer\n",
    "timeout_counter = 0\n",
    "timeout_limit = 10\n",
    "\n",
    "# Perform handshake with the consumer\n",
    "while True:\n",
    "    # ... (Handshake waiting code)\n",
    "    msg = handshake_consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        timeout_counter += 1\n",
    "        if timeout_counter >= timeout_limit:\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION FAILURE\")\n",
    "            print(\"==================================\")\n",
    "            exit(1)\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Handshake consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        handshake_msg = msg.value().decode('utf-8')\n",
    "        if handshake_msg == 'READY':\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION ESTABLISHED\")\n",
    "            print(\"==================================\")\n",
    "            break\n",
    "\n",
    "# Send a ready message to the consumer\n",
    "producer.produce('handshake', 'READY')\n",
    "\n",
    "# Iterate through the preprocessed data and send it to the Kafka producer line by line\n",
    "for i, row in X.iterrows():\n",
    "    #serialized_data = str(row)  # Convert the row to a string\n",
    "    serialized_data = ','.join(map(str, row.values))\n",
    "    print(f\"Serialized data: {serialized_data}\")\n",
    "    producer.produce('detect_anomalies', serialized_data)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Flush the producer to ensure all messages are sent\n",
    "producer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## `anomaly_detector.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anomaly_detector.py` is a Python script that performs the following tasks:\n",
    "\n",
    "1. Import necessary libraries and modules.\n",
    "2. Print a welcome message and wait for 1 second.\n",
    "3. Define the on_message function, which performs the following tasks:\n",
    "   1. Convert the received data to a list of strings using comma as the delimiter.\n",
    "   2. Append the list of strings to the data buffer.\n",
    "   3. Check if the data buffer has enough data points.\n",
    "   4. If the buffer has enough data points, convert the buffer to a NumPy array of floats, predict the anomalies using the pre-trained TensorFlow model, and check if any predicted values are above the threshold.\n",
    "   5. If an anomaly is detected, print an \"ANOMALY\" message. Otherwise, print a \"BENIGN\" message.\n",
    "   6. Reset the data buffer.\n",
    "4. Subscribe to the 'detect_anomalies' topic.\n",
    "5. Consume messages from the 'detect_anomalies' topic and process them using the on_message function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalay_detector.py\n",
    "# https://www.phind.com/search?cache=cf139efb-38e8-4fb5-9cda-5c67194a11a6\n",
    "\n",
    "from confluent_kafka import Consumer, Producer, KafkaError\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "# Print the header for the anomaly detector module\n",
    "print(\"==================================\")\n",
    "print(\"SPECTRE - CONSUMER & ANOMALY DETECTOR MODULE\")\n",
    "print(\"==================================\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Load the pre-trained TensorFlow model\n",
    "model = load_model('/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/A/spectre_ddos_2_h5.h5')\n",
    "\n",
    "# Print the header for the anomaly detector module\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'mygroup',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'queued.min.messages': 1  # Add this line to set the minimum number of records in the queue to 1\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(consumer_conf)\n",
    "\n",
    "# Define Kafka producer configuration for handshake with the consumer\n",
    "handshake_producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092'\n",
    "}\n",
    "\n",
    "# Create a Kafka producer instance for handshake\n",
    "handshake_producer = Producer(handshake_producer_conf)\n",
    "handshake_producer.produce('handshake', 'READY')\n",
    "\n",
    "# Define Kafka consumer configuration for handshake with the producer\n",
    "handshake_consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'consumer_handshake_group',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance for handshake\n",
    "handshake_consumer = Consumer(handshake_consumer_conf)\n",
    "handshake_consumer.subscribe(['handshake'])\n",
    "\n",
    "# Initialize timeout counter and limit for handshake\n",
    "timeout_counter = 0\n",
    "timeout_limit = 10\n",
    "\n",
    "# Perform handshake with the producer\n",
    "while True:\n",
    "    msg = handshake_consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        timeout_counter += 1\n",
    "        if timeout_counter >= timeout_limit:\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION FAILURE\")\n",
    "            print(\"==================================\")\n",
    "            exit(1)\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Handshake consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        handshake_msg = msg.value().decode('utf-8')\n",
    "        if handshake_msg == 'READY':\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION ESTABLISHED\")\n",
    "            print(\"==================================\")\n",
    "            break\n",
    "\n",
    "# Subscribe to the 'detect_anomalies' topic\n",
    "consumer.subscribe(['detect_anomalies'])\n",
    "\n",
    "# Initialize the data buffer\n",
    "received_data_buffer = []\n",
    "\n",
    "# Consume messages and process them using the on_message function\n",
    "def on_message(msg):\n",
    "    global received_data_buffer\n",
    "    # Choosing a higher threshold value (e.g., 0.7) will reduce the chances of benign data being misclassified as anomalies (false positives)\n",
    "    # but might also result in missing some actual anomalies (false negatives). The best threshold value balances the trade-off between false positives and false negatives.\n",
    "    # One approach to determine a good threshold value is to learn from past data, identifying the minimum and maximum deviations and setting the threshold accordingly, possibly with a scaling factor for flexibility.\n",
    "    threshold = 0.7  # Set the threshold value for anomaly detection\n",
    "    \n",
    "    \n",
    "    if msg.error():\n",
    "        print(f\"Consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        received_data_str = msg.value().decode('utf-8')  # Convert the received data to a string\n",
    "        received_data_list = received_data_str.strip('[]').split(',')  # Convert the received data string to a list of strings using comma as the delimiter\n",
    "        received_data_buffer.append(received_data_list)  # Append the list of strings to the buffer\n",
    "\n",
    "        if len(received_data_buffer) == 7:\n",
    "            X_received = np.array(received_data_buffer, dtype=np.float64)  # Convert the buffer to a numpy array of floats\n",
    "            prediction = model.predict(X_received)\n",
    "            print(f'Prediction: {prediction}')\n",
    "            \n",
    "            # Check if there is an anomaly and print the appropriate message\n",
    "            if np.any(prediction > threshold):\n",
    "                print(\"==================================\")\n",
    "                print(\"ANOMALY\")\n",
    "                print(\"==================================\")\n",
    "            else:\n",
    "                print(\"==================================\")\n",
    "                print(\"BENIGN\")\n",
    "                print(\"==================================\")\n",
    "                \n",
    "            received_data_buffer = []  # Reset the buffer\n",
    "        else:\n",
    "            # Debug: Print the received_data_str length\n",
    "            #print(f\"Received data length: {len(received_data_str)}\")  \n",
    "            \n",
    "            # Debug: Print the received_data_buffer\n",
    "            print(f\"Received data instances: {len(received_data_buffer)}\") \n",
    "\n",
    "# Consume messages and process them using the on_message function\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        on_message(msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the data\n",
    "def prod_datapreprocess(csv_file):\n",
    "    \n",
    "    # Read a CSV file and create a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    dimensions_num_for_PCA = 7\n",
    "    \n",
    "    # Function to clean the dataset by removing NaN, inf, and -inf values\n",
    "    def clean_dataset(df):\n",
    "        assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "        df.dropna(inplace=True)\n",
    "        indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "        return df[indices_to_keep]\n",
    "\n",
    "    # Function to get PCA feature names\n",
    "    def get_PCA_feature_names(num_of_pca_components):\n",
    "        feature_names = []\n",
    "        for i in range(num_of_pca_components):\n",
    "            feature_names.append(f\"Principal component {i+1}\")\n",
    "        return feature_names\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = clean_dataset(df_cleaned)\n",
    "\n",
    "    df_cleaned = df_cleaned.reset_index()\n",
    "    df_cleaned.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Saving the label attribute before dropping it\n",
    "    df_labels = df_cleaned['label']\n",
    "    df_cleaned.drop('label', axis=1, inplace=True)\n",
    "    df_features = df_cleaned.columns.tolist()\n",
    "\n",
    "    # Perform feature scaling\n",
    "    df_scaled = StandardScaler().fit_transform(df_cleaned)\n",
    "    df_scaled = pd.DataFrame(data=df_scaled, columns=df_features)\n",
    "\n",
    "    # Performing PCA\n",
    "    pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Creating a DataFrame with principal components\n",
    "    principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)\n",
    "    df_pc = pd.DataFrame(data=principal_components, columns=principal_component_headings)\n",
    "\n",
    "    # Combine the principal components with the original labels\n",
    "    df_final = pd.concat([df_pc, df_labels], axis=1)\n",
    "\n",
    "    # Perform label binarization. Converts \"ANOMALY\" = 1 and \"BENIGN\" = 0.\n",
    "    lb = LabelBinarizer()\n",
    "    df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "\n",
    "    # Split the dataset into features (X) and labels (y)\n",
    "    X = df_final.drop(['label'], axis = 1)\n",
    "    y = df_final['label']\n",
    "\n",
    "    # Returns features(X)\n",
    "    return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
