{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECTRE PROTOTYPE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `producer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`producer.py` is a Python script that performs the following tasks:\n",
    "\n",
    "1. Import necessary libraries and modules. \n",
    "2. Print a welcome message and wait for 1 second. \n",
    "3. Define the prod_datapreprocess function, which performs the following tasks: \n",
    "   1. Read a CSV file and create a DataFrame.\n",
    "   2. Clean the dataset by removing NaN, inf, and -inf values.\n",
    "   3. Perform feature scaling using StandardScaler.\n",
    "   4. Apply PCA (Principal Component Analysis) to reduce dimensionality.\n",
    "   5. Combine the principal components with the original labels.\n",
    "   6. Perform label binarization for the 'label' column.\n",
    "   7. Return the processed DataFrame with features (X) and labels (y).\n",
    "4. Call the prod_datapreprocess function on a given dataset.\n",
    "5. Configure the Kafka producer with the necessary settings.\n",
    "6. Create a Kafka producer instance.\n",
    "7. Configure and create a Kafka consumer for handshake purposes.\n",
    "8. Perform a handshake between the producer and consumer, waiting for a 'READY' message.\n",
    "9. Send a 'READY' message from the producer to the consumer.\n",
    "10. Iterate through the processed DataFrame (X) and send each row to the Kafka producer.\n",
    "11. Flush the producer to ensure all messages are sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# producer.py\n",
    "# https://www.phind.com/search?cache=cf139efb-38e8-4fb5-9cda-5c67194a11a6\n",
    "\n",
    "# Import necessary libraries\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "# Print the welcome message\n",
    "print(\"==================================\")\n",
    "print(\"SPECTRE - PRODUCER MODULE\")\n",
    "print(\"==================================\")\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Define a function to preprocess the data\n",
    "def prod_datapreprocess(csv_file):\n",
    "    \n",
    "    # Read a CSV file and create a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    dimensions_num_for_PCA = 7\n",
    "    \n",
    "    # Function to clean the dataset by removing NaN, inf, and -inf values\n",
    "    def clean_dataset(df):\n",
    "        assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "        df.dropna(inplace=True)\n",
    "        indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "        return df[indices_to_keep]\n",
    "\n",
    "    # Function to get PCA feature names\n",
    "    def get_PCA_feature_names(num_of_pca_components):\n",
    "        feature_names = []\n",
    "        for i in range(num_of_pca_components):\n",
    "            feature_names.append(f\"Principal component {i+1}\")\n",
    "        return feature_names\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = clean_dataset(df_cleaned)\n",
    "\n",
    "    df_cleaned = df_cleaned.reset_index()\n",
    "    df_cleaned.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Saving the label attribute before dropping it\n",
    "    df_labels = df_cleaned['label']\n",
    "    df_cleaned.drop('label', axis=1, inplace=True)\n",
    "    df_features = df_cleaned.columns.tolist()\n",
    "\n",
    "    # Perform feature scaling\n",
    "    df_scaled = StandardScaler().fit_transform(df_cleaned)\n",
    "    df_scaled = pd.DataFrame(data=df_scaled, columns=df_features)\n",
    "\n",
    "    # Performing PCA\n",
    "    pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Creating a DataFrame with principal components\n",
    "    principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)\n",
    "    df_pc = pd.DataFrame(data=principal_components, columns=principal_component_headings)\n",
    "\n",
    "    # Combine the principal components with the original labels\n",
    "    df_final = pd.concat([df_pc, df_labels], axis=1)\n",
    "\n",
    "    # Perform label binarization. Converts \"ANOMALY\" = 1 and \"BENIGN\" = 0.\n",
    "    lb = LabelBinarizer()\n",
    "    df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "\n",
    "    # Split the dataset into features (X) and labels (y)\n",
    "    X = df_final.drop(['label'], axis = 1)\n",
    "    y = df_final['label']\n",
    "\n",
    "    # Returns features(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "# Read the CSV file and preprocess the data\n",
    "\n",
    "# DDoS Attack CSV\n",
    "X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "\n",
    "# DDoS Prime CSV\n",
    "#X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/DDoS_Dataset/ddos_balanced/final_dataset.csv')\n",
    "\n",
    "# Bening CSV\n",
    "#X = prod_datapreprocess('/home/aryn/spectre-dev/dataset/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "\n",
    "# Set the producer configuration\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'max.in.flight.requests.per.connection': 1   # Add this line to set the maximum number of in-flight messages to 1\n",
    "}\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Set the handshake consumer configuration\n",
    "handshake_consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'producer_handshake_group',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer for the handshake\n",
    "handshake_consumer = Consumer(handshake_consumer_conf)\n",
    "handshake_consumer.subscribe(['handshake'])\n",
    "\n",
    "# Wait for the handshake from the consumer\n",
    "timeout_counter = 0\n",
    "timeout_limit = 10\n",
    "\n",
    "# Perform handshake with the consumer\n",
    "while True:\n",
    "    # ... (Handshake waiting code)\n",
    "    msg = handshake_consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        timeout_counter += 1\n",
    "        if timeout_counter >= timeout_limit:\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION FAILURE\")\n",
    "            print(\"==================================\")\n",
    "            exit(1)\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Handshake consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        handshake_msg = msg.value().decode('utf-8')\n",
    "        if handshake_msg == 'READY':\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION ESTABLISHED\")\n",
    "            print(\"==================================\")\n",
    "            break\n",
    "\n",
    "# Send a ready message to the consumer\n",
    "producer.produce('handshake', 'READY')\n",
    "\n",
    "# Iterate through the preprocessed data and send it to the Kafka producer line by line\n",
    "for i, row in X.iterrows():\n",
    "    #serialized_data = str(row)  # Convert the row to a string\n",
    "    serialized_data = ','.join(map(str, row.values))\n",
    "    print(f\"Serialized data: {serialized_data}\")\n",
    "    producer.produce('detect_anomalies', serialized_data)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Flush the producer to ensure all messages are sent\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## `anomaly_detector.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anomaly_detector.py` is a Python script that performs the following tasks:\n",
    "\n",
    "1. Import necessary libraries and modules.\n",
    "2. Print a welcome message and wait for 1 second.\n",
    "3. Define the on_message function, which performs the following tasks:\n",
    "   1. Convert the received data to a list of strings using comma as the delimiter.\n",
    "   2. Append the list of strings to the data buffer.\n",
    "   3. Check if the data buffer has enough data points.\n",
    "   4. If the buffer has enough data points, convert the buffer to a NumPy array of floats, predict the anomalies using the pre-trained TensorFlow model, and check if any predicted values are above the threshold.\n",
    "   5. If an anomaly is detected, print an \"ANOMALY\" message. Otherwise, print a \"BENIGN\" message.\n",
    "   6. Reset the data buffer.\n",
    "4. Subscribe to the 'detect_anomalies' topic.\n",
    "5. Consume messages from the 'detect_anomalies' topic and process them using the on_message function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalay_detector.py\n",
    "# https://www.phind.com/search?cache=cf139efb-38e8-4fb5-9cda-5c67194a11a6\n",
    "\n",
    "from confluent_kafka import Consumer, Producer, KafkaError\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "# Print the header for the anomaly detector module\n",
    "print(\"==================================\")\n",
    "print(\"SPECTRE - CONSUMER & ANOMALY DETECTOR MODULE\")\n",
    "print(\"==================================\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Load the pre-trained TensorFlow model\n",
    "model = load_model('/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/A/spectre_ddos_2_h5.h5')\n",
    "\n",
    "# Print the header for the anomaly detector module\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'mygroup',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'queued.min.messages': 1  # Add this line to set the minimum number of records in the queue to 1\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(consumer_conf)\n",
    "\n",
    "# Define Kafka producer configuration for handshake with the consumer\n",
    "handshake_producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092'\n",
    "}\n",
    "\n",
    "# Create a Kafka producer instance for handshake\n",
    "handshake_producer = Producer(handshake_producer_conf)\n",
    "handshake_producer.produce('handshake', 'READY')\n",
    "\n",
    "# Define Kafka consumer configuration for handshake with the producer\n",
    "handshake_consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'consumer_handshake_group',\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance for handshake\n",
    "handshake_consumer = Consumer(handshake_consumer_conf)\n",
    "handshake_consumer.subscribe(['handshake'])\n",
    "\n",
    "# Initialize timeout counter and limit for handshake\n",
    "timeout_counter = 0\n",
    "timeout_limit = 10\n",
    "\n",
    "# Perform handshake with the producer\n",
    "while True:\n",
    "    msg = handshake_consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        timeout_counter += 1\n",
    "        if timeout_counter >= timeout_limit:\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION FAILURE\")\n",
    "            print(\"==================================\")\n",
    "            exit(1)\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Handshake consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        handshake_msg = msg.value().decode('utf-8')\n",
    "        if handshake_msg == 'READY':\n",
    "            print(\"==================================\")\n",
    "            print(\"CONNECTION ESTABLISHED\")\n",
    "            print(\"==================================\")\n",
    "            break\n",
    "\n",
    "# Subscribe to the 'detect_anomalies' topic\n",
    "consumer.subscribe(['detect_anomalies'])\n",
    "\n",
    "# Initialize the data buffer\n",
    "received_data_buffer = []\n",
    "\n",
    "# Consume messages and process them using the on_message function\n",
    "def on_message(msg):\n",
    "    global received_data_buffer\n",
    "    # Choosing a higher threshold value (e.g., 0.7) will reduce the chances of benign data being misclassified as anomalies (false positives)\n",
    "    # but might also result in missing some actual anomalies (false negatives). The best threshold value balances the trade-off between false positives and false negatives.\n",
    "    # One approach to determine a good threshold value is to learn from past data, identifying the minimum and maximum deviations and setting the threshold accordingly, possibly with a scaling factor for flexibility.\n",
    "    threshold = 0.7  # Set the threshold value for anomaly detection\n",
    "    \n",
    "    \n",
    "    if msg.error():\n",
    "        print(f\"Consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        received_data_str = msg.value().decode('utf-8')  # Convert the received data to a string\n",
    "        received_data_list = received_data_str.strip('[]').split(',')  # Convert the received data string to a list of strings using comma as the delimiter\n",
    "        received_data_buffer.append(received_data_list)  # Append the list of strings to the buffer\n",
    "\n",
    "        if len(received_data_buffer) == 7:\n",
    "            X_received = np.array(received_data_buffer, dtype=np.float64)  # Convert the buffer to a numpy array of floats\n",
    "            prediction = model.predict(X_received)\n",
    "            print(f'Prediction: {prediction}')\n",
    "            \n",
    "            # Check if there is an anomaly and print the appropriate message\n",
    "            if np.any(prediction > threshold):\n",
    "                print(\"==================================\")\n",
    "                print(\"ANOMALY\")\n",
    "                print(\"==================================\")\n",
    "            else:\n",
    "                print(\"==================================\")\n",
    "                print(\"BENIGN\")\n",
    "                print(\"==================================\")\n",
    "                \n",
    "            received_data_buffer = []  # Reset the buffer\n",
    "        else:\n",
    "            # Debug: Print the received_data_str length\n",
    "            #print(f\"Received data length: {len(received_data_str)}\")  \n",
    "            \n",
    "            # Debug: Print the received_data_buffer\n",
    "            print(f\"Received data instances: {len(received_data_buffer)}\") \n",
    "\n",
    "# Consume messages and process them using the on_message function\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Consumer error: {msg.error()}\")\n",
    "    else:\n",
    "        on_message(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the data\n",
    "def prod_datapreprocess(csv_file):\n",
    "    \n",
    "    # Read a CSV file and create a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    dimensions_num_for_PCA = 7\n",
    "    \n",
    "    # Function to clean the dataset by removing NaN, inf, and -inf values\n",
    "    def clean_dataset(df):\n",
    "        assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "        df.dropna(inplace=True)\n",
    "        indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "        return df[indices_to_keep]\n",
    "\n",
    "    # Function to get PCA feature names\n",
    "    def get_PCA_feature_names(num_of_pca_components):\n",
    "        feature_names = []\n",
    "        for i in range(num_of_pca_components):\n",
    "            feature_names.append(f\"Principal component {i+1}\")\n",
    "        return feature_names\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = clean_dataset(df_cleaned)\n",
    "\n",
    "    df_cleaned = df_cleaned.reset_index()\n",
    "    df_cleaned.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Saving the label attribute before dropping it\n",
    "    df_labels = df_cleaned['label']\n",
    "    df_cleaned.drop('label', axis=1, inplace=True)\n",
    "    df_features = df_cleaned.columns.tolist()\n",
    "\n",
    "    # Perform feature scaling\n",
    "    df_scaled = StandardScaler().fit_transform(df_cleaned)\n",
    "    df_scaled = pd.DataFrame(data=df_scaled, columns=df_features)\n",
    "\n",
    "    # Performing PCA\n",
    "    pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Creating a DataFrame with principal components\n",
    "    principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)\n",
    "    df_pc = pd.DataFrame(data=principal_components, columns=principal_component_headings)\n",
    "\n",
    "    # Combine the principal components with the original labels\n",
    "    df_final = pd.concat([df_pc, df_labels], axis=1)\n",
    "\n",
    "    # Perform label binarization. Converts \"ANOMALY\" = 1 and \"BENIGN\" = 0.\n",
    "    lb = LabelBinarizer()\n",
    "    df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "\n",
    "    # Split the dataset into features (X) and labels (y)\n",
    "    X = df_final.drop(['label'], axis = 1)\n",
    "    y = df_final['label']\n",
    "\n",
    "    # Returns features(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2b$12$RbKiXOZRKYyqx3dBmt4VdeViVTL6EY.yLO5wV1TnUXtgJEtbGhuEu', '$2b$12$DSkNtQJytUsuPwPFjLbrtOdroqBgGnFFyKcHdz0vu0./ZB1Pt6F/O']\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import streamlit_authenticator as stauth\n",
    "hashed_passwords = stauth.Hasher(['admin1', 'password1']).generate()\n",
    "print(hashed_passwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$2b$12$D0e7eVJNHDIFArw3un.IG.W0Clty3hgALoY56SxCRaW41XzABpCCm\n"
     ]
    }
   ],
   "source": [
    "import bcrypt\n",
    "\n",
    "password = \"admin1\".encode('utf-8')\n",
    "hashed_password = bcrypt.hashpw(password, bcrypt.gensalt()).decode('utf-8')\n",
    "print(hashed_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from confluent_kafka.admin import AdminClient\n",
    "from streamlit_extras.colored_header import colored_header\n",
    "from streamlit_extras.metric_cards import style_metric_cards\n",
    "from streamlit import date_input\n",
    "from datetime import datetime\n",
    "import streamlit_authenticator as stauth\n",
    "import plotly.express as px\n",
    "import altair as alt\n",
    "import bcrypt\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"/home/aryn/spectre-dev/spectre-code/spectre-ann/prototype/database/predictions.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"SPECTRE DDoS Detection Dashboard\",\n",
    "    page_icon=\"üß†\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"collapsed\",\n",
    ")\n",
    "\n",
    "# Add a collapsible sidebar for the refresh button\n",
    "with st.sidebar:\n",
    "    st.header(\"üß† SPECTRE Options\")\n",
    "    #st.write(\"Click the button below to refresh the data:\")\n",
    "    st.caption(\"Refresh SPECTRE Dashboard\")\n",
    "    refresh_button = st.button(\"Refresh Data\")\n",
    "    st.divider()\n",
    "    st.subheader(\"‚ö†Ô∏è DANGEROUS\")\n",
    "    st.caption(\"Careful when using these options\")\n",
    "    del_db = st.button(\"Delete Database Entry\")\n",
    "    \n",
    "    \n",
    "# dashboard title\n",
    "st.title(\"üß† SPECTRE DASHBOARD\")\n",
    "st.caption(\"A lightweight solution for DDoS Detection\")\n",
    "\n",
    "# Create a placeholder for the line chart\n",
    "line_chart_placeholder = st.empty()\n",
    "\n",
    "# Function to fetch data from the database and display it\n",
    "def display_predictions():\n",
    "    \n",
    "    # Define Kafka configuration\n",
    "    kafka_conf = {\n",
    "        'bootstrap.servers': 'localhost:9092'\n",
    "    }\n",
    "\n",
    "    # Create an AdminClient instance\n",
    "    admin_client = AdminClient(kafka_conf)\n",
    "\n",
    "    # Get the metadata for the Kafka cluster\n",
    "    metadata = admin_client.list_topics(timeout=5)\n",
    "\n",
    "    #st.header(\"DASHBOARD\")\n",
    "       \n",
    "    # Query the predictions from the database\n",
    "    c.execute('SELECT * FROM predictions')\n",
    "    predictions = c.fetchall()\n",
    "\n",
    "    # Convert the predictions to a pandas DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['ID', 'Prediction', 'Result', 'F1 Score', 'Timestamp'])\n",
    "    #st.write(f\"Unique values in Result column: {predictions_df['Result'].unique()}\")\n",
    "\n",
    "    # Convert the Timestamp column to a DatetimeIndex\n",
    "    #anomaly_counts = predictions_df[predictions_df['Result'] == 'ANOMALY'].set_index('Timestamp').resample('5T').count()\n",
    "    \n",
    "    # Convert the 'Timestamp' column to a pandas datetime object\n",
    "    predictions_df['Timestamp'] = pd.to_datetime(predictions_df['Timestamp'])\n",
    "    \n",
    "    # Set the 'Timestamp' column as the index and resample\n",
    "    # Group the predictions_df by 'Result' and resample by 'Timestamp'\n",
    "    grouped_df = predictions_df.groupby('Result').resample('5T', on='Timestamp').count()\n",
    "\n",
    "    # Filter the grouped_df for 'ANOMALY' and reset the index\n",
    "    anomaly_counts = grouped_df.loc['ANOMALY'].reset_index()\n",
    "\n",
    "        \n",
    "    # Query the count of anomalies and benign results from the database\n",
    "    c.execute(\"SELECT Result, COUNT(*) FROM predictions GROUP BY Result\")\n",
    "    count_data = dict(c.fetchall())\n",
    "    #st.write(f\"Total rows in predictions table: {len(predictions)}\")  # Add this line to check the values in the Result column\n",
    "\n",
    "    st.divider()\n",
    "    \n",
    "    col_top1, col_top2 = st.columns(2)\n",
    "    \n",
    "    with col_top1:\n",
    "        #st.subheader(\"Attack Summary\")\n",
    "        colored_header(\n",
    "            label=\"Attack Summary\",\n",
    "            description=\"A summary of attacks that occured\",\n",
    "            color_name=\"yellow-80\",\n",
    "        )\n",
    "        # Create a row for the metrics\n",
    "        #metrics_row = st.columns(2)\n",
    "        \n",
    "        # Display the count of anomalies and benign results using st.metric\n",
    "        #with metrics_row[0]:\n",
    "        #    st.metric(\"DDoS Count\", count_data.get('ANOMALY', 0))\n",
    "        #with metrics_row[1]:\n",
    "        #    st.metric(\"Benign Count\", count_data.get('BENIGN', 0))\n",
    "        \n",
    "        met_col1, met_col2 = st.columns(2)\n",
    "        met_col1.metric(label=\"DDoS Count\",value=count_data.get('ANOMALY', 0))\n",
    "        met_col2.metric(label=\"Benign Count\",value=count_data.get('BENIGN', 0))\n",
    "        style_metric_cards(background_color=\"#191923\", border_left_color= \"#E59500\", border_color=\"#E59500\", border_size_px=2, border_radius_px= 5)\n",
    "        \n",
    "    with col_top2:\n",
    "        #st.subheader(\"SPECTRE Details\")\n",
    "        colored_header(\n",
    "            label=\"SPECTRE Details\",\n",
    "            description=\"Overview on SPECTRE\",\n",
    "            color_name=\"yellow-80\",\n",
    "        )\n",
    "        kafka_expander = st.expander(label='KAFKA METRICS')\n",
    "        with kafka_expander:\n",
    "            #st.subheader(\"Welcome to Developer Area\")\n",
    "            # Kafka Information\n",
    "            #st.write(\"KAFKA METRICS\")  \n",
    "            # Create a container to display the number of topics\n",
    "            with st.container():\n",
    "                # Check if there are any errors\n",
    "                if not metadata.brokers:\n",
    "                    st.warning(\"Kafka is not running properly!\")\n",
    "                else:\n",
    "                    st.success(\"Kafka is running properly!\")\n",
    "        status_expander = st.expander(label='SPECTRE Status')\n",
    "        with status_expander:\n",
    "            #st.write(\"SPECTRE Status\")\n",
    "            st.write(\"Version: 2.0\")\n",
    "\n",
    "    st.divider()\n",
    "    \n",
    "    # Add the date_input widget to the date_col\n",
    "    selected_date = date_input(\"Select a Date\", value=datetime.today().date())\n",
    "    \n",
    "    # Create two columns for displaying the table and line chart side by side\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # Display the table in the first column\n",
    "    with col1:\n",
    "        #st.subheader(\"Log Details\")\n",
    "        colored_header(\n",
    "            label=\"Log Details\",\n",
    "            description=\"Attack Logs\",\n",
    "            color_name=\"yellow-80\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # Filter the DataFrame based on the selected date\n",
    "        filtered_df = predictions_df[predictions_df['Timestamp'].dt.date == selected_date]\n",
    "        \n",
    "        # Add a select box to choose between top 10 results and all results\n",
    "        table_option = st.selectbox(\"Choose table display option:\", [\"Recent 10 Results\", \"All Results\"])\n",
    "\n",
    "        # Filter the DataFrame based on the selected option\n",
    "        if table_option == \"Recent 10 Results\":\n",
    "            #display_df = predictions_df.tail(10)\n",
    "            display_df = filtered_df.tail(10)\n",
    "        else:\n",
    "            #display_df = predictions_df\n",
    "            display_df = filtered_df\n",
    "\n",
    "        # Set the option to display all columns without truncation\n",
    "        pd.set_option('display.max_columns', None)\n",
    "\n",
    "        # Display the table with a scrollable container and full width\n",
    "        st.dataframe(display_df[['Timestamp', 'F1 Score', 'Result']], use_container_width=True, hide_index=True)\n",
    "\n",
    "    # Line Chart Definition\n",
    "    # Create a new DataFrame for the line chart with separate columns for anomalies and benign predictions\n",
    "    #line_chart_data = predictions_df[predictions_df['Result'] == 'ANOMALY'].set_index('Timestamp').resample('5S').count()['Result']\n",
    "    # Display the line chart in the second column\n",
    "    with col2:\n",
    "        #st.subheader(\"Attack Graph\")\n",
    "        #st.write(\"This line chart shows the number of anomalies over time:\")\n",
    "        colored_header(\n",
    "            label=\"Attack Graph\",\n",
    "            description=\"This line chart shows the number of anomalies over time\",\n",
    "            color_name=\"yellow-80\",\n",
    "        )\n",
    "                      \n",
    "        # Add a 'Date' column to the anomaly_counts DataFrame\n",
    "        anomaly_counts['Date'] = anomaly_counts['Timestamp'].dt.date\n",
    "\n",
    "\n",
    "        # Filter the anomaly_counts DataFrame based on the selected date\n",
    "        anomaly_counts_filtered = anomaly_counts[anomaly_counts['Date'].astype(str) == str(selected_date)]\n",
    "        \n",
    "        st.write(\"Anomaly Counts Filtered DataFrame:\")\n",
    "        st.write(anomaly_counts_filtered)\n",
    "        st.write(print(\"Timestamp data:\", anomaly_counts_filtered['Timestamp'].tolist()))\n",
    "        st.write(print(\"Anomaly count data:\", anomaly_counts_filtered['ID'].tolist()))\n",
    "\n",
    "        st.write(\"Grouped DataFrame:\")\n",
    "        st.write(grouped_df)\n",
    "        \n",
    "        st.write(\"Anomaly Counts DataFrame:\")\n",
    "        st.write(anomaly_counts)\n",
    "        \n",
    "        if anomaly_counts_filtered.empty:\n",
    "            st.warning(\"No data available for the selected date.\")\n",
    "        else:\n",
    "            fig = px.line(anomaly_counts_filtered, x='Timestamp', y='ID', title='Anomalies Over Time')\n",
    "            fig.update_xaxes(title_text='Timestamp')\n",
    "            fig.update_yaxes(title_text='Anomaly Count')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    dev_expander = st.expander(label='Developer Area')\n",
    "    with dev_expander:\n",
    "        st.header(\"Welcome to Developer Area\")\n",
    "        st.caption(\"Components to added\")\n",
    "        \n",
    "            \n",
    "    \n",
    "# Refresh the data when the refresh button is clicked\n",
    "if refresh_button:\n",
    "    display_predictions()\n",
    "elif del_db:\n",
    "    # Delete all rows from the predictions table\n",
    "    c.execute(\"DELETE FROM predictions\")\n",
    "    conn.commit()\n",
    "else:\n",
    "    # Display the data automatically every 2 minutes (120 seconds)\n",
    "    while True:\n",
    "        display_predictions()\n",
    "        time.sleep(120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
