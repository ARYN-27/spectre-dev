{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECTRE-CPU-V2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP PRE-REQUISITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "from keras import layers, models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import seaborn as sns # Graphing, built ontop of MatPlot for ease-of-use and nicer diagrams.\n",
    "import sklearn # For Machine Learning algorithms\n",
    "import scikitplot # Confusion matrix plotting\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.model_selection import train_test_split # For basic dataset splitting\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\n",
    "from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\n",
    "from sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance\n",
    "from sklearn.neural_network import MLPClassifier # For Neural Network classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def escape():\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT SETUP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup INFO level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n",
    "# Only 7 principal components needed when using non-normalised PCA dataset.\n",
    "dimensions_num_for_PCA = 7\n",
    "\n",
    "# Max number of permutations to run. Can be altered for needs.\n",
    "number_of_permutations = 100\n",
    "\n",
    "# 10 folds is usually the heuristic to follow for larger datasets of around this size.\n",
    "num_of_splits_for_skf = 10\n",
    "\n",
    "# Seed value to pass into models so that repeated runs result in the same output\n",
    "seed_val = 1\n",
    "\n",
    "# Number of statistical distance measures to run (for the results, columns section)\n",
    "num_of_statistical_dist_measures = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHODS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Clean_dataset()` method is used to remove infinite and Nan value errors (in the original dataset), which can cause issues in the PCA transform step.\n",
    "- `assert` keyword can be used for testing purposes. If the input param is not of type Pandas Dataframe, then the error message will be shown. Thus, you 'assert' that a pd.Dataframe is input.\n",
    "- `.dropna(inplace=True)` drops any rows which contain NaN/ Null values. NaN values would cause issues later on, with model training and other functions. A lot of functions require NaN values to be removed to work (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
    "\n",
    "Code reference: https://stackoverflow.com/a/46581125 (with a minor change = removed the conversion to float64 type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_PCA_feature_names() `method is used to generate feature names for the number of PCA components passed in as a param. Returns a list of feature names for principal component column headings, in a Pandas Dataframe.\n",
    "- No special code here, just pure Python code. Instantiating a list, and populating it with strings for the PCA header names. Then, appending each to the list and returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA_feature_names(num_of_pca_components):\n",
    "    feature_names = []\n",
    "    for i in range(num_of_pca_components):    \n",
    "        feature_names.append(f\"Principal component {i+1}\")\n",
    "    return feature_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN TRAINING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Friday_Morning_Data = pd.read_csv('../../dataset/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "df = Friday_Morning_Data.copy()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing column name issues**\n",
    "\n",
    "Because of Excel being used to create the csv, the column headings/ names contain whitespace padding, incorrect capitalisation, etc... which makes it difficult to correctly select by column names. This piece of code below just removes these issues.\n",
    "\n",
    "- df_columns can be used to access all the column heading names (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html)\n",
    "- The code below acts on each of the header names (for ease of use):\n",
    "  - str.strip() just strips/ removes any leading and trailing spaces\n",
    "  - str.lower() just converts all characters to lower case\n",
    "  - str.replace('x', 'y') just replaces all instances of x with y, i.e. changing spaces to '_'\n",
    "  - str.replace('x', '') can be used to remove the specified x characters (replacing with nothing/ empty char)\n",
    "Code Reference: https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datatset Data Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing issues with ScikitLearn's PCA transform on this dataset**\n",
    "\n",
    "Without cleaning the dataset, the PCA transform will throw this error:\n",
    "- \"sklearn error ValueError: Input contains NaN, infinity or a value too large for dtype('float64')\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "df_cleaned = clean_dataset(df_cleaned) # see methods at top of notebook\n",
    "df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resetting indexes since rows have been dropped. See the difference between above dataframe indexes and below.**\n",
    "- `.reset_index()` method resets the Pandas Dataframe indexes, for the rows. Useful to do after removing rows, as this messes up the indexes. It creates a new 'index' column that needs to be dropped, as it's useless (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html)\n",
    "- `.drop([column_name], axis=x, inplace=y)` drops a specified column from the dataframe and returns a new copy. When `inplace=True`, it transforms the Dataframe itself (instead of needing to copy). The `axis` specifies the dimension of what to drop i.e. if **axis=0, it drops a row. If axis=1, it drops a column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.reset_index()\n",
    "# Removing un-needed index column added by reset_index method\n",
    "df_cleaned.drop('index', axis=1, inplace=True)\n",
    "df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What features should be included from PCA, and why?**\n",
    "\n",
    "Looking at the list of feature names in the dataset (shown below), one can see that all other features should be of numeric type (with domain knowledge). They're all currently numeric type (either float or int). Consequently, PCA can be fully applied.\n",
    "- `df.columns.tolist()` converts the Dataframe column names into a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation: PCA Dimension reduction and scaling (Hughes' Phenomenon)**\n",
    "\n",
    "PCA acts to reduce the dimensions/ search space of the dataset as much as possible, while trying to maintain the most information possible e.g. It can easily reduce the dimensionality by more than half, while still maintaining 99% of the original data's information- it does this by extracting out the most important information/ trends/ spread (variance) of each dimension/ attribute- into n 'principal components'.\n",
    "\n",
    "More formally: PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n",
    "\n",
    ">Key note:\n",
    "> > \"PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.\" (https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "\n",
    "PCA still works without standardizing the features to unit scale but tranforming to unit scale should still be done to prevent large variance features from having an over-bearing affect on other lower variance features (via something like StandardScaler here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "This is particularly important with this dataset, as some features have massively wide variances and others do not (e.g. the 'idle_std' values can range from e+06, all the way to zero).\n",
    "\n",
    "- `Dataframe['x']` syntax allows the selection of one of the Dataframe columns (where 'x' is the column heading/ name).\n",
    "- `df_Series.unique()` returns an array, showing all the unique values inside the Pandas Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the label attribute before dropping it.\n",
    "df_labels = df_cleaned['label']\n",
    "# Shows all the possible labels/ classes a model can predict.\n",
    "# Need to alter these to numeric 0, 1, etc... for model comprehension (e.g. pd.get_dummies()).\n",
    "df_labels.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label column has to be removed as you don't want this involved in the PCA process. It can be concatted back with the PCA tranformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.\n",
    "df_no_labels = df_cleaned.drop('label', axis=1, inplace=False)\n",
    "# Getting feature names for the StandardScaler process\n",
    "df_features = df_no_labels.columns.tolist()\n",
    "# Printing out Dataframe with no label column, to show successful dropping\n",
    "df_no_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using StandardScaler to transform features into unit scale (optional for PCA)**\n",
    "\n",
    "- `StandardScaler()` is an imported model from the sklearn.preprocessing library. It scales the specified Pandas Dataframe or Series object values to unit scale/ variance. This is usually required for certain functions to perform correctly, e.g. the PCA transform later (see docs: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "- `StandardScaler().fit_transform(df)` fits the StandardScaler model to the data, and transforms it into unit scale.\n",
    "- `pd.Dataframe(data=x, columns=y)` can convert the data 'x' into a Pandas Dataframe object, using the respective columns 'y'\n",
    "\n",
    "Code references: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = StandardScaler().fit_transform(df_no_labels)\n",
    "# Converting back to dataframe\n",
    "df_scaled = pd.DataFrame(data = df_scaled, columns = df_features)\n",
    "df_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting and transforming the data with PCA**\n",
    "\n",
    "Thus, the optimal number of principle components is set to the environment variable and this is now used to produce the appropriate multi-dimensional principle component array. This will be formatted back to a Pandas dataframe afterwards.\n",
    "\n",
    "References: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html and https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "#principal_components = pca.fit(df_scaled).transform(df_scaled) => for normalised PCA\n",
    "\n",
    "# Non-normalised PCA\n",
    "principal_components = pca.fit(df_no_labels).transform(df_no_labels)\n",
    "principal_components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Principal Component feature names, dynamically**\n",
    "\n",
    "Getting the Principal Component feature names, dynamically, for the optimal number of components (passed in as a param). Allows dynamic changing of PCs used.\n",
    "\n",
    "> i.e. One can change the 'dimensions_num_for_PCA' environment variable, and all the code will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Methods at the top of the notebook\n",
    "principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the Principal Components back into a Pandas Dataframe, ready for concatting back with the label feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc = pd.DataFrame(data = principal_components, columns = principal_component_headings)\n",
    "df_pc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining/ concatinating the label feature back onto the pca transformed dataset.**\n",
    "\n",
    "Label still needs to be transformed into binary data (for model comprehension/ understanding i.e. the model doesn't understand string data but string data can be transformed into numeric data, which is model can understand and use).\n",
    "\n",
    "See the Pandas.concat docs here: https://pandas.pydata.org/docs/reference/api/pandas.concat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_pc, df_labels], axis = 1)\n",
    "# Scroll to the RHS end of dataframe to see attached label feature\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming the label feature's categorical data into numeric data (via LabelBinarizer)**\n",
    "\n",
    "Again, a model can't understand, for example, 'yes' and 'no' strings but... these can be mapped to a 1 for yes and a 0 for no. Then a model can understand, as it requires numeric input to distinguish the feature's domain of values.\n",
    "\n",
    "The `sklearn.preprocessing.LabelBinarize`r can be used to convert the column data into binary numbers, which will then be correctly interpreted.\n",
    "\n",
    "- Fit the List- this tells the LabelBinarizer what values exist, and how to map them.\n",
    "- Call transform, passing a List, and this will return the encoded List.\n",
    "\n",
    "> (Note: if label column has more than 2 unique labels, pandas.get_dummies is required instead)\n",
    "\n",
    "(Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the transformation. Again, to note, if label isn't binary then pd.get_dummies is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before LabelBinarizer: \", df_labels.unique())\n",
    "print(\"After LabelBinarizer: \", df_final['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation and Stratified splitting**\n",
    "\n",
    "K-Fold is a technique which splits data into K folds (splits). Train of a model K times, and for each training iteration, K-Fold selects a different fold to use for testing; the remaining K - 1 folds become the training data. Typically, the optimal K value can be derived using the size of your dataset (num of rows). Ideally, each fold should be statistically representative of the population. Too small and it won't be useful. Too large, and you lose the positives from doing K-Fold.\n",
    "\n",
    "You can use Stratified splitting with K-Fold, which ensures balance between some criteria (balances out the classes) e.g. equal portion of label classes in each fold.\n",
    "\n",
    "Class Imbalance is a significant issue in the ML/ Data Mining domain. It leads to incorrect results e.g. if one fold had all of 1 label (accidentally), then it would produce terrible predictive results as it wouldn't know what the other label class data point would look like. You can only work with the data you have, so this has to be dealt with.\n",
    "\n",
    "Benefits of K-Fold:\n",
    "- Use more of the data towards making a succesful model.\n",
    "- Obtain K models to evaluate, can improve the confidence that you have selected an appropriate model algorithm and cleaned/ prepared the data correctly, e.g. normal split with 1 model, one doesn't know if it's good or not- it could be heavily biased. Multiple models ensures less bias and increased variance.\n",
    "- Looking at the accuracy results from each of the k-Folds, you can identify data issues e.g. a certain fold performs really badly. Could this suggest that more cleaning is required? Maybe the data preparation was performed incorrectly?\n",
    "- If all folds return similar accuracies, one can be more confident that a deployed model will perform similarly to how one expects.\n",
    "\n",
    "Issues with K-Fold:\n",
    "- Creating K separate models requires more computation.\n",
    "- If you haven't got much data, you might not get many folds. Less folds means K-Fold loses its benefits.\n",
    "- If K is very large, each fold is small, and harder to ensure statistical distribution of.\n",
    "- Choosing the best of K models introduces bias. Real world data could perform better under a more general, lower performing model.\n",
    "\n",
    "Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the label so that the answers aren't provided to the model, in training.\n",
    "X = df_final.drop(['label'], axis = 1)\n",
    "y = df_final['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\n",
    "skf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, splitting the data into train and test data, using the optimal splitting techniques of K-Fold and Stratified Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "    \n",
    "print( 'X_train length: ', len(X_train) ) # To check if splits worked\n",
    "print( 'y_train length: ', len(y_train) )\n",
    "print( 'X_test length: ', len(X_test) )\n",
    "print( 'y_test length: ', len(y_test) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Define the ANN model\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "#model = Sequential([\n",
    "#    Dense(512, kernel_initializer='he_normal', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(256, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(64, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(32, kernel_initializer='he_normal', kernel_regularizer=l2(0.001)),\n",
    "#    LeakyReLU(alpha=0.1),\n",
    "#    BatchNormalization(),\n",
    "#    Dropout(0.4),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, kernel_initializer='he_normal', input_shape=(X_train.shape[1],), kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate performance metrics\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as SavedModel\n",
    "tf.saved_model.save(model, '/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/SavedModel/')\n",
    "\n",
    "# Export as Keras Model\n",
    "model.save(\"/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/spectre_ddos_2_hd5\")\n",
    "\n",
    "# Export as Keras H5 Model\n",
    "model.save(\"/home/aryn/spectre-dev/spectre-code/spectre-ann/Model/DDOS_2/spectre_ddos_2_h5.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN ACTIVATION GRAPH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
